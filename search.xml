<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>写paper的小工具</title>
      <link href="/2024/12/10/%E5%86%99%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B8%AE%E5%8A%A9/"/>
      <url>/2024/12/10/%E5%86%99%E8%AE%BA%E6%96%87%E7%9B%B8%E5%85%B3%E7%9A%84%E5%B8%AE%E5%8A%A9/</url>
      
        <content type="html"><![CDATA[<h1 id="一些自用小工具"><a href="#一些自用小工具" class="headerlink" title="一些自用小工具"></a>一些自用小工具</h1><ol><li><p><a href="https://github.com/ICRDoge/ICR-blender">Blender</a><br>这个用来做可视化效果会好看一些，链接是我写的教程。如果换作机械臂或者其他机器人的话，其原理是一样的，不过需要花费一些时间。<br><img src="/img/climb.png" alt="Blender效果图"></p></li><li><p><a href="https://www.remove.bg/">抠图网站</a><br>做framework的时候想加一些截取的图片，但是背景颜色和自己的底色差的太多，可以用抠图软件去掉背景。<br><img src="/img/%E6%8A%A0%E5%9B%BEex.png" alt="抠图效果"></p></li><li><p><a href="https://blog.csdn.net/weixin_42782150/article/details/104878759">Markdown语法大全</a><br>这个网站我随便在百度上找的，如果有更全更好看的可以麻烦告诉我一下，谢谢。</p></li><li><p><a href="https://github.com/dair-ai/ml-visuals">神经网络相关配图示例</a><br>这个也是我在网上随便找的，虽然这个仓库的示例比较卡通，不太符合我的风格，但是确实给我了很多思路。<br><img src="/img/MLvisual.png" alt="MLvisual"></p></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>2024.11.7组会分享</title>
      <link href="/2024/11/03/2024-11-7%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB/"/>
      <url>/2024/11/03/2024-11-7%E7%BB%84%E4%BC%9A%E5%88%86%E4%BA%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="Versatile-Multi-Contact-Planning-and-Control-for-Legged-Loco-Manipulation"><a href="#Versatile-Multi-Contact-Planning-and-Control-for-Legged-Loco-Manipulation" class="headerlink" title="Versatile Multi-Contact Planning and Control for Legged Loco-Manipulation"></a><strong>Versatile Multi-Contact Planning and Control for Legged Loco-Manipulation</strong></h1><h2 id="JEAN-PIERRE-SLEIMAN-FARBOD-FARSHIDIAN-AND-MARCO-HUTTER"><a href="#JEAN-PIERRE-SLEIMAN-FARBOD-FARSHIDIAN-AND-MARCO-HUTTER" class="headerlink" title="JEAN-PIERRE SLEIMAN, FARBOD FARSHIDIAN, AND MARCO HUTTER"></a><em>JEAN-PIERRE SLEIMAN, FARBOD FARSHIDIAN, AND MARCO HUTTER</em></h2><h2 id="Science-Robotics-2023"><a href="#Science-Robotics-2023" class="headerlink" title="Science Robotics 2023"></a>Science Robotics 2023</h2><span id="more"></span><blockquote><p>本博客是我自己的讲解稿，如有错误，望及时提醒，也希望对大家能够有一些帮助，谢谢大家！</p></blockquote><p>大家好，我今天要分享的论文是《Versatile Multi-Contact Planning and Control for Legged Loco-Manipulation》，这是一篇关于狗加臂的文章。<br>移动操作在我们的日常生活中的作用日益显著，目前人们已经可以使移动操作机器人完成一些短期的（处理单一场景）简单的移动或者操作任务，然而完成一系列长序列多接触多功能的移动操作任务是比较困难的。<br>先前的方法包括：强化学习，虽然消除了对精确模型的需求，但是人工设计奖励函数和较长的训练时间仍然困扰着人们，对这一方法的改进是使用模仿学习，但是每一个专家轨迹只对应特定的任务，无法实现其通用性。另外一种方法是轨迹优化（TO），但是移动操作是一个混合规划问题（决策变量的离散和连续），所以计算会非常困难。而且比较依赖于问题的初始化、局部最小值问题等等。基于采样的方法和图搜索方法（如运动基元 RRT等）只能针对特定任务进行设计，而无法保证通用性。<br>作者发现，大多数方法都可以归纳为TAMP的一个实例，本文提出了一种通用的移动操作方法，在预先建模的环境下，给定对象模型和任务定义下，我们的规划器会告诉机器人应该如何行动，应该施加什么力，应该使用哪些肢体，解决了长序列的移动操作任务。在本文的工作中，能够生成并自动组合运动和操作的wbc，并不需要依赖动作基元。<br>本文的方法包含三个部分，分别是用户输入，离线规划器和在线跟随模块。<br>在用户输入部分，我们需要定义机器人和目标的模型，任务描述和affordance。也就是确定一下任务的规则，比如开门任务中，门上哪些部分是可抓握的或者是可以用足端推动的。<br>在离线规划器中，我们使用基于采样的图搜索方法来找到可行但是次优的轨迹，并将求解的可行轨迹用于热启动轨迹优化来后处理得到连续最优轨迹。在前半部分中，我们定义了一个双层最优化问题，在外层，我们将状态s定义为机器人肢体的接触状态，动作表示为肢体接触状态的改变。在内层优化中，我们使用较复杂的动力学模型来建模（最小但高保真SRB），输入机器人和目标物体的状态（关节角位姿等），输出末端执行器的接触力和关节角速度。在双层优化问题中，成本函数的定义很简单，包括惩罚大输入，跟踪参考轨迹等等，都是比较通用的cost。这里基于采样是对于计算成本函数来说的，在外层中，每个节点初始化一个参数化的cost，使用ANA*算法来选择成本最低的节点进行扩展，快速找到一个可行解，然后根据参数的变化收敛到A*算法来增强解。在采样过程中，我们分为了随机扩展和以目标为导向的扩展。<br>虽然一个由多个平滑轨迹组成的序列（即一个轨迹规划方案）在其单独的部分上都是平滑的，但整体上却可能不平滑。这尤其会发生在轨迹序列的生成过程中存在随机因素的情况下。所以我们使用轨迹优化后处理来增强轨迹的质量。目的是平滑轨迹并消除多余的action。这会导致更小的力，更鲁棒的行为，提高了任务的成功性和稳定性。<br>最后是我们的在线跟踪模块，对于静态环境，设计一个跟踪器是很容易的，但是我们的环境是动态的，所以为了保证安全性和减小误差，对于离线规划器计算出来的轨迹，我们使用了两层的控制器来跟踪。第一层是以跟踪轨迹为主要目的的MPC，他在这里充当一个滤波器的作用，对于不符合规定的动作轨迹进行筛除，并且这个MPC是独立于任务的。第二层是一个全身控制器，用来跟踪MPC输出的关节角及角速度。<br>对于实验结果，我们可以看到机器人能够完全自主地完成任务，并能够在不同的接触模式中进行切换。通过查看曲线我们可以看到，我们的离线规划器是很智能的。</p><h3 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h3><blockquote><p><a href="https://arxiv.org/abs/2308.09179">arxiv</a><br><a href="https://drive.google.com/file/d/1QN0QLcCOsNPksJuLs9YNa3kM2n1SzlL2/view?usp=drive_link">ppt</a><br><a href="https://www.youtube.com/watch?v=rAP7M4BL9sQ&t=163s">Youtube</a></p></blockquote>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Getting Started with Quadruped Robots</title>
      <link href="/2024/10/30/Getting-Started-with-Quadruped-Robots/"/>
      <url>/2024/10/30/Getting-Started-with-Quadruped-Robots/</url>
      
        <content type="html"><![CDATA[<h1 id="四足入门该学什么"><a href="#四足入门该学什么" class="headerlink" title="四足入门该学什么"></a>四足入门该学什么</h1><span id="more"></span><blockquote><p>这个页面是给准备学习四足知识的学弟学妹们准备的<br>有些地方只是<a href="mailto:yhykid@mail.ustc.edu.cn">我</a>自己的理解，可能会有错误，希望大家及时指正，谢谢！</p></blockquote><h2 id="常见平台"><a href="#常见平台" class="headerlink" title="常见平台"></a>常见平台</h2><details><summary><b>常见的四足机器人</b></summary><ul><li><a href="https://www.aidinrobotics.co.kr/leggedrobot-aidin">AiDIN</a></li><li><a href="https://rsl.ethz.ch/robots-media/anymal.html">ANYmal</a></li><li><a href="https://ai.googleblog.com/2023/05/barkour-benchmarking-animal-level.html">Barkour</a></li><li><a href="https://biomimetics.mit.edu/">Cheetah</a></li><li><a href="https://www.mi.com/cyberdog2">CyberDog 2</a></li><li><a href="https://www.deeprobotics.cn/en/index/product.html">DeepRobotics</a></li><li><a href="https://robots.ieee.org/robots/hyq/">HyQ</a></li><li><a href="https://magnecko.ethz.ch/">Magnecko</a></li><li><a href="https://www.chinamoneynetwork.com/2021/03/05/tencents-new-robotic-dog-max-has-wheels-on-its-knees-can-travel-at-25km-hour">Max</a></li><li><a href="https://github.com/haraduka/mevius">Mevius</a></li><li><a href="https://rclab.kookmin.ac.kr/project1/quadruped-robot">PongBot</a></li><li><a href="https://www.railab.kaist.ac.kr/sections/robots#raibo-2">Raibo</a></li><li><a href="https://www.rainbow-robotics.com/en_rbq">RBQ</a></li><li><a href="https://open-dynamic-robot-initiative.github.io/">Solo8</a></li><li><a href="https://www.bostondynamics.com/products/spot">Spot</a></li><li><a href="https://www.unitree.com/">Unitree</a></li><li><a href="https://www.ghostrobotics.io/vision-60">Vision60</a></details></li></ul><details><summary><b>我们实验室使用的四足</b></summary><ul><li><a href="https://www.unitree.com/cn/a1">unitree A1</a></li><li><a href="https://www.unitree.com/cn/go2">unitree Go2</a></details></li></ul><hr><h2 id="理论基础"><a href="#理论基础" class="headerlink" title="理论基础"></a>理论基础</h2><details><summary><b>图书</b></summary><ul><li><a href="https://www.bilibili.com/read/cv23222491/">机器人学导论（实验室有)</a></li><li><a href="https://item.jd.com/10111410465673.html">现代机器人学（实验室有)</a></details></li></ul><details><summary><b>控制方法</b></summary><ul><li><a href="https://zhuanlan.zhihu.com/p/99409532">Model Predictive Control(MPC)</a></li><li><a href="https://yeekal.store/notes/motionPlanning/20_trajectory_optimization">Trajectory Optimization(TO)</a></li><li>Reinforcement Learning(RL)</details></li></ul><details><summary><b>paper</b></summary><ul><li><a href="https://arxiv.org/abs/2109.11978">Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning（CoRL 2021)</a></li><li><a href="https://arxiv.org/abs/2107.04034">RMA: Rapid Motor Adaptation for Legged Robots（RSS 2021)</a></li><li><a href="https://sites.google.com/view/saferlleggedlocomotion/">Safe Reinforcement Learning for Legged Locomotion(IROS 2022)</a></li><li><a href="https://gmargo11.github.io/walk-these-ways/">Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior(CoRL 2022)</a></li><li><a href="https://extreme-parkour.github.io/">Extreme Parkour with Legged Robots(ICRA 2024)</a></li><li><a href="https://arxiv.org/abs/1804.10332">Sim-to-Real: Learning Agile Locomotion For Quadruped Robots</a></li><li><a href="https://www.science.org/doi/10.1126/scirobotics.abc5986">Learning quadrupedal locomotion over challenging terrain(Science Robotics 2020)</a></li><li><a href="https://arxiv.org/abs/2103.00946">A Unified MPC Framework for Whole-Body Dynamic Locomotion and Manipulation(RAL 2021)</a></li><li><a href="https://arxiv.org/abs/2210.10044">Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion(CoRL 2022)</a></li><li><a href="https://agile-but-safe.github.io/">Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion(RSS 2024)</a></li><li><a href="https://www.science.org/doi/10.1126/scirobotics.adh5401">DTC: Deep Tracking Control(Science Robotics 2024)</a></li><li><a href="https://umi-on-legs.github.io/">UMI on Legs: Making Manipulation Policies Mobile with Manipulation-Centric Whole-body Controllers(CoRL 2024)</a></li><li><a href="https://leggedrobotics.github.io/viplanner.github.io/">ViPlanner: Visual Semantic Imperative Learning for Local Navigation</a></li><li><a href="https://naoki.io/portfolio/vlfm">VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation(ICRA 2024)</a></li><li><a href="https://arxiv.org/abs/2301.10602">DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning(ICRA 2023)</a></li><li><a href="https://github.com/OpenRobotLab/HIMLoco">Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response(ICLR 2024)</a></li><li><a href="https://lucidsim.github.io/">Learning Visual Parkour from Generated Images(CoRL 2024)</a></li><li><a href="https://lecar-lab.github.io/dial-mpc/">Full-Order Sampling-Based MPC for Torque-Level Locomotion Control via Diffusion-Style Annealing</a></li><li><a href="https://locomanip-duet.github.io/">RoboDuet: A Framework Affording Mobile-Manipulation and Cross-Embodiment</a></li></ul></details><h2 id="一些比较好用的仓库"><a href="#一些比较好用的仓库" class="headerlink" title="一些比较好用的仓库"></a>一些比较好用的仓库</h2><details><summary><b>仿真平台</b></summary><ul><li><a href="https://github.com/leggedrobotics/legged_gym">IsaacGym</a>:这个是四足RL最常用的平台，如果后续要开发的话，建议详细地看一下<a href="https://developer.nvidia.com/isaac-gym">Nvidia官方的文档</a></li><li><a href="https://github.com/isaac-sim/IsaacLab">IsaacSim</a>:对于配置要求较高，没系统地用过，后面打算把工作迁移到这里</li><li><a href="https://github.com/google-deepmind/mujoco">mujoco</a>:没系统地使用过</details></li></ul><details><summary><b>算法</b></summary><ul><li><a href="https://github.com/NVlabs/curobo">Curobo</a>:前两天用这个算机械臂的逆运动学来着，并行，又快又准</details></li></ul><hr>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
